# -*- coding: utf-8 -*-
"""Review of Scientific papers.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g1hrZPa6WtL44o1p2YrEg_ddbv6gnKYx
    

# TOPIC MODELLING
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

import matplotlib.pyplot as plt
import seaborn as sns
import nltk
import re
import string
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# Gensim
import gensim
import gensim.corpora as corpora
from gensim.utils import simple_preprocess
from gensim.models import CoherenceModel
from pprint import pprint

# Plotting tools
import subprocess
import sys

subprocess.check_call([sys.executable, "-m", "pip", "install", "pyLDAvis"])
subprocess.check_call([sys.executable, "-m", "pip", "install", "matplotlib"])

import pyLDAvis
import pyLDAvis.gensim_models as gensimvis

import warnings
warnings.filterwarnings("ignore",category=DeprecationWarning)

"""## Exploring the dataset"""

medium_articles = pd.read_csv("articles.csv")
medium_articles.head()

article_text = medium_articles.text
# visualise first article print few words from it
#print("First 1000 words in first article are : \n",article_text[0][:1000])

#total number of author
print(f"Total Number of unique authors : {len(medium_articles.author.unique())}")

#counting the articles of different authors
article_counts = dict()
for author in medium_articles.author:
    article_count = len(medium_articles[medium_articles["author"]==author])
    article_counts[author] = article_count

#sorting the article_counts on the basis of article count
article_counts = dict(sorted(article_counts.items(), key=lambda x: x[1], reverse=True))


#plot histogram of top 10 authors and their article counts
plt.figure(figsize=(14,7))
plt.subplot(1, 2, 1)
sns.barplot(x = list(article_counts.keys())[:10],y=list(article_counts.values())[:10])
plt.title("Authors and Number of Articles")
plt.xlabel("Authors")
plt.ylabel("No. of Articles")
plt.xticks(rotation = 90)



#counting the total number of claps for each author
def convert_to_num(clap_count):
    if "K" in clap_count:
        if "." in clap_count:
            clap_count = re.sub("\.","",clap_count[:-1])+"00"
        else:
            clap_count = clap_count[:-1]+"000"
    return(int(clap_count))

medium_articles.claps = medium_articles.claps.apply(convert_to_num)

#couting total number of claps for each author
clap_counts = dict()
for author in medium_articles.author:
    clap_count = medium_articles[medium_articles["author"]==author]["claps"]
    clap_counts[author] = sum(clap_count)

#sort clap_counts on the basis of counting of claps
clap_counts = dict(sorted(clap_counts.items(), key=lambda x: x[1], reverse=True))

#plot histogram of top 10 authors and their article counts
plt.subplot(1, 2, 2)
sns.barplot(x = list(clap_counts.keys())[:10],y=list(clap_counts.values())[:10])
plt.title("Authors and Number of Claps")
plt.xlabel("Authors")
plt.ylabel("No. of Claps")
plt.xticks(rotation = 90)
plt.show()

"""The above graphs show that Adam Getigey has written most articles and has the highest number of claps. But when it comes to the 2nd, 3rd positions, the author's name is not the same in the graphs. It shows that either the reach of articles or the quality of articles is inadequate for the authors.

## Preprocessing Text Data
"""

#lowercase the articles
medium_articles.text = medium_articles.text.apply(lambda t : t.lower())

"""Expanding contractions: Contractions are the shortened form of the words like it's, hasn't. We expand them for better analysis of our text data.
I have taken these contractions from Analytics Vidhay's article.
"""

# Dictionary of English Contractions
contractions_dict = { "ain't": "are not","'s":" is","aren't": "are not",
                     "can't": "cannot","can't've": "cannot have",
                     "'cause": "because","could've": "could have","couldn't": "could not",
                     "couldn't've": "could not have", "didn't": "did not","doesn't": "does not",
                     "don't": "do not","hadn't": "had not","hadn't've": "had not have",
                     "hasn't": "has not","haven't": "have not","he'd": "he would",
                     "he'd've": "he would have","he'll": "he will", "he'll've": "he will have",
                     "how'd": "how did","how'd'y": "how do you","how'll": "how will",
                     "I'd": "I would", "I'd've": "I would have","I'll": "I will",
                     "I'll've": "I will have","I'm": "I am","I've": "I have", "isn't": "is not",
                     "it'd": "it would","it'd've": "it would have","it'll": "it will",
                     "it'll've": "it will have", "let's": "let us","ma'am": "madam",
                     "mayn't": "may not","might've": "might have","mightn't": "might not",
                     "mightn't've": "might not have","must've": "must have","mustn't": "must not",
                     "mustn't've": "must not have", "needn't": "need not",
                     "needn't've": "need not have","o'clock": "of the clock","oughtn't": "ought not",
                     "oughtn't've": "ought not have","shan't": "shall not","sha'n't": "shall not",
                     "shan't've": "shall not have","she'd": "she would","she'd've": "she would have",
                     "she'll": "she will", "she'll've": "she will have","should've": "should have",
                     "shouldn't": "should not", "shouldn't've": "should not have","so've": "so have",
                     "that'd": "that would","that'd've": "that would have", "there'd": "there would",
                     "there'd've": "there would have", "they'd": "they would",
                     "they'd've": "they would have","they'll": "they will",
                     "they'll've": "they will have", "they're": "they are","they've": "they have",
                     "to've": "to have","wasn't": "was not","we'd": "we would",
                     "we'd've": "we would have","we'll": "we will","we'll've": "we will have",
                     "we're": "we are","we've": "we have", "weren't": "were not","what'll": "what will",
                     "what'll've": "what will have","what're": "what are", "what've": "what have",
                     "when've": "when have","where'd": "where did", "where've": "where have",
                     "who'll": "who will","who'll've": "who will have","who've": "who have",
                     "why've": "why have","will've": "will have","won't": "will not",
                     "won't've": "will not have", "would've": "would have","wouldn't": "would not",
                     "wouldn't've": "would not have","y'all": "you all", "y'all'd": "you all would",
                     "y'all'd've": "you all would have","y'all're": "you all are",
                     "y'all've": "you all have", "you'd": "you would","you'd've": "you would have",
                     "you'll": "you will","you'll've": "you will have", "you're": "you are",
                     "you've": "you have"}

# Regular expression for finding contractions
contractions_re=re.compile('(%s)' % '|'.join(contractions_dict.keys()))

# Function for expanding contractions
def expand_contractions(text,contractions_dict=contractions_dict):
    def replace(match):
        return contractions_dict[match.group(0)]
    return contractions_re.sub(replace, text)

# Expanding Contractions in the text data
medium_articles.text = medium_articles.text.apply(lambda x:expand_contractions(x))

"""Removing Stopwords and Puctuations: Stopwords are the words that are highly occurred in a language; they do not add any significance to the sentence. "a," "an," "the," "in" are some examples of stopwords. So we generally ignore stop words during our NLP task. In some NLP tasks, stop words are also important, but stopwords are unnecessary for topic modeling. We also remove punctuation marks because they are also unnecessary and do not contribute anything."""

import nltk
nltk.download('stopwords')

from nltk.corpus import stopwords
print(stopwords.words('english'))

print(f"These are the pucntions which string.punctuation consist :  {string.punctuation}")

from nltk.corpus import stopwords
import string

# Load stopwords
stop_words = set(stopwords.words('english'))

# Function to remove stopwords
def remove_stopwords(text):
    # Tokenize the text and filter out stopwords
    words = text.split()
    filtered_words = [word for word in words if word.lower() not in stop_words]
    return ' '.join(filtered_words)

# Ensure 'text' column exists
if 'text' in medium_articles.columns:
    medium_articles['text'] = medium_articles['text'].apply(remove_stopwords)
else:
    print("The column 'text' does not exist in the dataset.")

print(medium_articles['text'].dtype)

medium_articles['text'] = medium_articles['text'].fillna('')

print(medium_articles['text'].head())

print(medium_articles['text'].apply(remove_stopwords).head())

import subprocess
import sys

subprocess.check_call([sys.executable, "-m", "pip", "install", "nltk"])
import nltk
nltk.download('punkt_tab')

stop_words = set(stopwords.words('english'))

def remove_stopwords(article):
    "Return the article after remvoing stopwords"
    article_tokens = word_tokenize(article)
    filtered_article = [word for word in article_tokens if not word in stop_words]
    return " ".join(filtered_article)


#removing stopwords
medium_articles.text = medium_articles.text.apply(remove_stopwords)

#removing Punctuations
medium_articles.text = medium_articles.text.apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '', x))

#removing digits
medium_articles.text = medium_articles.text.apply(lambda x: re.sub('\w*\d\w*','', x))

medium_articles.text[50]

"""There are still some marks like quotation marks, hypen and apostrophe still remaining in the article, we also need to remove them."""

def remove_extra_marks(article):
    extra_keys = ["’","—","”","“"]
    article_tokens = word_tokenize(article)
    filtered_article = [word for word in article_tokens if not word in extra_keys]
    return " ".join(filtered_article)

medium_articles.text = medium_articles.text.apply(remove_extra_marks)

#printing arbitrary example to visulise clean data
medium_articles.text[50]

"""Lemmaztisation: Lemmatization process Topic modeling algorithm to avoid making mistakes by converting the word in its root from like lemmatization prevents counting "help", "helps", "helping" as three different words. Stemming can also be used, but it performs the task more forcefully, and sometimes the word loses its meaning, so I prefer lemmatization for the topic modeling task.

Here I am using spacy library to perform the lemmatizaton task.
"""

import spacy
nlp = spacy.load('en_core_web_sm')
print(f""" "helps" after lemmztization :  {nlp("helps")[0].lemma_}""")
print(f""" "helping" after lemmztization :  {nlp("helping")[0].lemma_}""")

def lemmatize(text):
    """Return text after performing the lemmztiztion"""
    doc = nlp(text)
    tokens = [token for token in doc]
    return  " ".join([token.lemma_ for token in doc])

#lemmatize the articles
medium_articles.text = medium_articles.text.apply(lemmatize)

"""Now we have finished with all the data preprocessing steps, and let's plot a word cloud to visualize the most frequently used words and see the most commonly occurred word in the articles."""

import wordcloud
from wordcloud import WordCloud

#combine all the articles
article_data = ""
for article in medium_articles.text:
    article_data = article_data+" "+article

#ploting the word cloud
plt.figure(figsize=(10, 10))
wordcloud = WordCloud(width = 500, height = 500, background_color='#40E0D0', colormap="ocean",  random_state=10).generate(article_data)
plt.imshow(wordcloud)
plt.axis("off")
plt.show()

"""## Latent Dirichlet Allocation[LDA]
LDA algorithm is an unsupervised learning algorithm that works on a probabilistic statistical model to discover topics that the document contains automatically.
This algorithm works on certain assumptions. The assumption is that the document was generated by picking a set of topics and picking a set of words for each topic. In other words, we can say that the document is a probability distribution over the latent topics, and topics are probability distribution over the words.
With the help of this LDA, we try to estimate the words that belong to each
topic and find the topics in documents accordingly.
"""

#tokenize articles
tokeize_article = medium_articles.text.apply(lambda x : x.split())
id2word = corpora.Dictionary(tokeize_article)

# Create Corpus
texts = tokeize_article

# Term Document Frequency
corpus = [id2word.doc2bow(text) for text in texts]


#printing 50 words from the text corpus
corpus_example = [[(id2word[id], freq) for id, freq in cp] for cp in corpus[:2]]
corpus_example[0][:50]

# build LDA model for 10 topic
lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                           id2word=id2word,
                                           num_topics=10,
                                           random_state=100,
                                           update_every=1,
                                           chunksize=100,
                                           passes=10,
                                           alpha='symmetric',
                                           per_word_topics=True,
                                           eta = 0.6)

# Print the Keyword in the 10 topics
pprint(lda_model.print_topics())
doc_lda = lda_model[corpus]

"""We can interpret this output as for topic 0, word, vector,ai, make, cluster, go, system, use, one are 10 most important keywords. The weight of "ai" on topic 0 is 0.008

### Visualising Topics
"""

# Visualize the topics
pyLDAvis.enable_notebook()
vis = gensimvis.prepare(lda_model, corpus, id2word)
vis

"""To measure how good is our model we can use perplexity score and coherence socre. The lower score of perplexity is better for the model."""

# Compute Perplexity
print('\nPerplexity : ', lda_model.log_perplexity(corpus))

# Compute Coherence Score
coherence_model_lda = CoherenceModel(model=lda_model, texts=tokeize_article, dictionary=id2word, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
print('\nCoherence Score: ', coherence_lda)

"""### Tuning hyperparameter

We can try out different number of topics, different values of alpha and beta(eta) to increse the conharence score. High conherence score is good for our model.
"""

def calculate_coherence_score(n, alpha, beta):
    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                           id2word=id2word,
                                           num_topics=n,
                                           random_state=100,
                                           update_every=1,
                                           chunksize=100,
                                           passes=10,
                                           alpha=alpha,
                                           per_word_topics=True,
                                           eta = beta)
    coherence_model_lda = CoherenceModel(model=lda_model, texts=tokeize_article, dictionary=id2word, coherence='c_v')
    coherence_lda = coherence_model_lda.get_coherence()
    return coherence_lda

#list containing various hyperparameters
no_of_topics = [2,5,7,10,12,14]
alpha_list = ['symmetric',0.3,0.5,0.7]
beta_list = ['auto',0.3,0.5,0.7]


for n in no_of_topics:
    for alpha in alpha_list:
        for beta in beta_list:
            coherence_score = calculate_coherence_score(n, alpha, beta)
            print(f"n : {n} ; alpha : {alpha} ; beta : {beta} ; Score : {coherence_score}")

"""The highest coherence score is : 0.4962 for number of topics = 14 alpha = 0.3 and beta = auto

Final LDA Model
"""

n = 14
alpha = 0.3
beta = "auto"
lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                           id2word=id2word,
                                           num_topics=n,
                                           random_state=100,
                                           update_every=1,
                                           chunksize=100,
                                           passes=10,
                                           alpha=alpha,
                                           per_word_topics=True,
                                           eta = beta)
coherence_model_lda = CoherenceModel(model=lda_model, texts=tokeize_article, dictionary=id2word, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
print('\nCoherence Score: ', coherence_lda)

# Visualize the topics
pyLDAvis.enable_notebook()
vis = gensimvis.prepare(lda_model, corpus, id2word)
vis

"""# ABSTRACT CLUSTERING"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
nechbamohammed_research_papers_dataset_path = kagglehub.dataset_download('nechbamohammed/research-papers-dataset')

print('Data source import complete.')

"""## Importing and Checking the dataset"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install bertopic
# !pip install minisom

from plotly.offline import init_notebook_mode
init_notebook_mode(connected=True)

import pandas as pd                                        #Data processing, CSV files I/O (e.g. pd.read_csv)
import numpy as np                                         #Linear Algebra: Matrices ...
import matplotlib.pyplot as plt                            #Data Visualisation
import seaborn as sns
from bertopic import BERTopic

from tqdm import tqdm
# I discoverd that it's possible to download models for the specific purpose to preprocess scientific texts
# In the spacy docs I found a specific model for this : https://spacy.io/universe/project/scispacy
#Downloading en_core_sci_lg model to preprocess abstracts
from IPython.utils import io

import os

# Install SciSpacy model if not already installed
os.system("pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_lg-0.4.0.tar.gz")


#Import NLP librarys and the spacy package to preprocess the abstract text
import spacy
from spacy.lang.en.stop_words import STOP_WORDS #import commen list of stopword
import en_core_sci_lg  # import downlaoded model
import string
from minisom import MiniSom
from sklearn.cluster import SpectralClustering
import scipy.cluster.hierarchy as sch
from sklearn.decomposition import PCA
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import silhouette_score

df =  pd.read_csv("dblp-v10.csv")
df.head(5)

df.info()

df.isna().sum()

"""## Data Cleaning"""

df.dropna(subset='abstract',inplace=True)

df.isna().sum()

"""## NLP Data Preprocessing"""

# Parser
parser = en_core_sci_lg.load()
parser.max_length = 7000000 #Limit the size of the parser

def spacy_tokenizer(sentence):
    ''' Function to preprocess text of scientific papers
        (e.g Removing Stopword and puntuations)'''
    mytokens = parser(sentence)
    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != "-PRON-" else word.lower_ for word in mytokens ] # transform to lowercase and then split the scentence
    mytokens = [ word for word in mytokens if word not in stopwords and word not in punctuations ] #remove stopsword an punctuation
    mytokens = " ".join([i for i in mytokens])
    return mytokens

punctuations = string.punctuation #list of punctuation to remove from text
stopwords = list(STOP_WORDS)
stopwords[:10]

# the dataframe contains still hugh amount of data. The process the data faster I reduce the df to 10000 rows
# The scope of the notebook is not to analyze all data
df = df.sample(10000, random_state=42)

tqdm.pandas()
df["processed_text"] = df["abstract"].progress_apply(spacy_tokenizer)

df['abstract']

"""## Model Training :BERtopic"""

topic_model = BERTopic(verbose=True, embedding_model="paraphrase-MiniLM-L6-v2", min_topic_size=50)
topics, _ = topic_model.fit_transform(df["processed_text"].to_numpy()); len(topic_model.get_topic_info())

"""### a. Topic representation"""

topic_model.get_topic_info().head(10)

topic_model.visualize_barchart(top_n_topics=9, height=700)

topic_model.visualize_term_rank()

"""### b. Topic Relationships"""

topic_model.visualize_topics(top_n_topics=21)

topic_model.visualize_hierarchy(top_n_topics=21, width=800)

topic_model.visualize_heatmap(n_clusters=5, top_n_topics=21)

"""## Clustering: Uncovering Patterns

### a. Vectorization of the abstract and dimensionality reduction with PCA
"""

# define vec function
def vectorize(text, maxx_features):

    vectorizer = TfidfVectorizer(max_features=maxx_features)
    X = vectorizer.fit_transform(text)
    return X

#vectorize each processed abstract
text = df['processed_text'].values
X = vectorize(text, 2 ** 12) #arbitrary max feature -_> Hyperpara. for optimisation (?)
X.shape

pca = PCA(n_components=0.95, random_state=42) #Keep 95% of the variance
X_reduced= pca.fit_transform(X.toarray())
X_reduced.shape

"""### b. Hierarchial Clustering"""

# Calculer la matrice de liaison avec la méthode "ward"
linkage_matrix = sch.linkage(X_reduced, method='ward')

# Créer un modèle de clustering agglomératif
clustering = AgglomerativeClustering(n_clusters=3,linkage='ward')

# Ajuster le modèle aux données réduites
clustering.fit(X_reduced)

# Prédire les clusters pour chaque échantillon
labels = clustering.labels_

# Calculer le score de silhouette
silhouette_avg = silhouette_score(X_reduced, labels)

print("Score de silhouette moyen :", silhouette_avg)

plt.figure(1, figsize = (16 ,8))
dendrogram = sch.dendrogram(linkage_matrix)

plt.title('Dendrogram')
plt.xlabel('Abstract')
plt.ylabel('Euclidean distances')
plt.show()

"""### c. Spectral Clustering"""

# Reducing the dimensions of the data
pca = PCA(n_components = 2)
X_principal = pca.fit_transform(X.toarray())
X_principal = pd.DataFrame(X_principal)
X_principal.columns = ['P1', 'P2']

# Building the clustering model
spectral_model_rbf = SpectralClustering(n_clusters = 2, affinity ='rbf')

# Training the model and Storing the predicted cluster labels
labels_rbf = spectral_model_rbf.fit_predict(X_principal)

# Visualizing the clustering
plt.scatter(X_principal['P1'], X_principal['P2'],
           c = SpectralClustering(n_clusters = 2, affinity ='rbf') .fit_predict(X_principal), cmap =plt.cm.winter)
plt.show()

"""### d. Self Organizing Maps (SOM)"""

X_principal = X_principal.to_numpy()

# I use linear som topography
som_shape = (1, 5)

som = MiniSom(som_shape[0], som_shape[1], X_principal.shape[1], sigma=0.5, learning_rate=0.5)

max_iter = 1000
q_error = []
t_error = []

for i in range(max_iter):
    rand_i = np.random.randint(len(X_principal))
    som.update(X_principal[rand_i], som.winner(X_principal[rand_i]), i, max_iter)
    q_error.append(som.quantization_error(X_principal))
    t_error.append(som.topographic_error(X_principal))

plt.plot(np.arange(max_iter), q_error, label='quantization error')
plt.plot(np.arange(max_iter), t_error, label='topographic error')
plt.ylabel('Quantization error')
plt.xlabel('Iteration index')
plt.legend()
plt.show()

# each neuron represents a cluster
winner_coordinates = np.array([som.winner(x) for x in X_principal]).T

# with np.ravel_multi_index we convert the bidimensional
# coordinates to a monodimensional index
cluster_index = np.ravel_multi_index(winner_coordinates, som_shape)

# Plotting the clusters
plt.figure(figsize=(10,8))

for c in np.unique(cluster_index):
    plt.scatter(X_principal[cluster_index == c, 0],
                X_principal[cluster_index == c, 1], label='cluster='+str(c), alpha=.7)

# Plotting centroids
for centroid in som.get_weights():
    plt.scatter(centroid[:, 0], centroid[:, 1], marker='x',
                s=10, linewidths=20, color='k') # label='centroid'

plt.title("Clusters of Abstract")
plt.xlabel("P1")
plt.ylabel("P2")
plt.legend();

"""# CITATION NETWORK ANALYSIS"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
mathurinache_citation_network_dataset_path = kagglehub.dataset_download('mathurinache/citation-network-dataset')

print('Data source import complete.')

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

import subprocess
import sys

subprocess.check_call([sys.executable, "-m", "pip", "install", "bigjson"])
subprocess.check_call([sys.executable, "-m", "pip", "install", "pycairo"])
subprocess.check_call([sys.executable, "-m", "pip", "install", "python-igraph"])

import bigjson
from igraph import *
import igraph as ig
# Import pycairo instead of cairo
import cairo # cairo is now installed
import matplotlib.pyplot as plt

"""## Data Preparation"""

# Download the dataset from Kaggle
import subprocess

subprocess.run(['kaggle', 'datasets', 'download', '-d', 'mathurinache/citation-network-dataset'])


# Unzip the dataset
subprocess.run(['unzip', 'citation-network-dataset.zip', '-d', '/content/'])
import bigjson
import numpy as np

# Initialize a dictionary to store references
references_dict = {
    'id': [],
    'title': [],
    'doc_type': [],
    'publisher': [],
    'venue_name': [],
    'venue_ID': [],
    'references': []
}

# Correct file path for the unzipped file
file_path = '/content/dblp.v12.json'

# Open the JSON file using bigjson to avoid memory issues
with open(file_path, 'rb') as f:
    j = bigjson.load(f)
    count = 0
    while count < 130:  # Adjust this for the number of records you want to process
        element = j[count]
        if 'references' in element.keys():
            for i, val in enumerate(element['references']):
                references_dict['references'].append(element['references'][i])
                references_dict['id'].append(element['id'])
                references_dict['title'].append(element['title'])

                # Handle missing values for publisher and doc_type
                references_dict['publisher'].append(element['publisher'] if element['publisher'] != "" else np.nan)
                references_dict['doc_type'].append(element['doc_type'] if element['doc_type'] != "" else np.nan)

                # Handle 'venue' and its nested keys
                if 'venue' in element.keys():
                    references_dict['venue_name'].append(element['venue'].get('raw', np.nan))
                    references_dict['venue_ID'].append(element['venue'].get('id', np.nan))
                else:
                    references_dict['venue_name'].append(np.nan)
                    references_dict['venue_ID'].append(np.nan)
        else:
            # Handle cases where no references exist
            references_dict['references'].append(np.nan)
            references_dict['id'].append(element['id'])
            references_dict['title'].append(element['title'])

            references_dict['publisher'].append(element['publisher'] if element['publisher'] != "" else np.nan)
            references_dict['doc_type'].append(element['doc_type'] if element['doc_type'] != "" else np.nan)

            # Handle 'venue' and its nested keys
            if 'venue' in element.keys():
                references_dict['venue_name'].append(element['venue'].get('raw', np.nan))
                references_dict['venue_ID'].append(element['venue'].get('id', np.nan))
            else:
                references_dict['venue_name'].append(np.nan)
                references_dict['venue_ID'].append(np.nan)

        count += 1  # Increment the count to process the next element

# You can now process or view the collected references data
print(references_dict)

import pandas as pd # import the pandas library and alias it as pd

data = pd.DataFrame.from_dict(references_dict)
data.head(5)

"""I will keep unique values in the id column and non-null values in references colunmn along with corresponding values of doc_type column"""

reference_doc = pd.concat([data.loc[:,['id', 'doc_type']].drop_duplicates(), data[data['references'].notna()].loc[:, ['references', 'doc_type']].rename(columns = {'references': 'id'})],axis = 0).sort_values(by=['id'])
reference_list = list(reference_doc['id'].values)
doc_list = list(reference_doc['doc_type'].values)
count = len(reference_list)

import subprocess
import sys

subprocess.check_call([sys.executable, "-m", "pip", "install", "python-igraph"])

references_graph = Graph(directed = False)
references_graph.add_vertices(count)
references_graph.vs['id_reference'] = reference_list
references_graph.vs['doc_type'] = doc_list

def add_edge(x):
    """
    This function adds an edge (u,v) with:
    u = index of 'id' vertex
    v = index of 'references' vertex, with duplicate values being considered different
    """
    references_graph.add_edge(reference_list.index(x.id),reference_list.index(x.references))
    return x.id,x.references

temp = data[data['references'].notna()].apply(add_edge, axis = 1)

"""## **Network Visualizations**
* The graph below shows a small network of publications (number = 130) along with their references
* Color indicates the 'doc_type' of the publication; each reference has been colored with the same color as its source i.e., the concered publication
* As expected, majority of references are refered only once in the small subset; a few have around 2 publications refering them
"""

import subprocess
import sys

subprocess.check_call([sys.executable, "-m", "pip", "install", "matplotlib"])
import matplotlib.pyplot as plt # Import the pyplot module from matplotlib as plt
import numpy as np # Import numpy for np.nan
import igraph as ig

color_dict = {"Conference": "blue",
              "Journal": "red",
              "Repository": "green",
               np.nan: "purple",
              'Patent': 'orange',
              'Other': 'black'}

x = list(str(i) for i in color_dict.keys())
y = [5 for i in range(len(x))]
fig = plt.figure(figsize = (3,1.5))
ax1 = plt.axes(frameon=False)
ax1.axes.get_xaxis().set_visible(False)
plt.barh(x,y,
         color = list(color_dict.values()))
plt.show()

references_graph.vs["color"] = [color_dict.get(doc_type,'black') for doc_type in references_graph.vs["doc_type"]]
ig.plot(references_graph, # Use ig.plot from the igraph library
        vertex_size=7)

"""# Research Paper Recommendation

**Aim:**
* Extracting Research Papers data from Arxiv dataset
* Using Universal Sentence Encoder to extract embeddings of Research Abstracts
* Training a K Neighbors Classifier to find similar research papers

## Setup
"""

import warnings
warnings.filterwarnings('ignore')

import os
import json
import random
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import tensorflow as tf
import tensorflow_hub as hub
from tensorflow.keras.losses import cosine_similarity
from sklearn.decomposition import PCA
from sklearn.neighbors import KNeighborsClassifier

plt.rcParams['figure.figsize'] = (8,5)
plt.rcParams['font.size'] = 14

"""## Loading Data"""

# Download the dataset
import subprocess

subprocess.run(['kaggle', 'datasets', 'download', '-d', 'Cornell-University/arxiv'])

# Unzip the dataset
import subprocess

subprocess.run(['unzip', 'arxiv.zip'])


import json # Import the json module
import warnings
warnings.filterwarnings('ignore')

import os
# ... other imports ...


FILE = 'arxiv-metadata-oai-snapshot.json'

def get_data():
    with open(FILE) as f:
        for line in f:
            yield line

dataframe = {
    'title': [],
    'year': [],
    'abstract': []
}

data = get_data()
for i, paper in enumerate(data):
    paper = json.loads(paper) # Now json.loads should work correctly
    try:
        date = int(paper['update_date'].split('-')[0])
        if date > 2019:
            dataframe['title'].append(paper['title'])
            dataframe['year'].append(date)
            dataframe['abstract'].append(paper['abstract'])
    except: pass

import pandas as pd
df = pd.DataFrame(dataframe)
df.head(10)

del dataframe

print(f"Samples: {df.shape[0]}\nFeatures: {df.shape[1]}")

any(df.isna().sum())

df.info()

"""## Exploratory Data Analysis"""

import seaborn as sns
import matplotlib.pyplot as plt
sns.countplot(data=df, x='year')
plt.title("Papers Released across Years")
plt.show()

"""### Abstract Length"""

df['length'] = df['abstract'].str.len()
df.head(10)

sns.boxplot(data=df, y='length')
plt.title("Length of Abstracts")
plt.show()

"""### Word Count"""

def word_count(x):
    return len(x.split())

df['word_count'] = df['abstract'].apply(word_count)
df.head()

sns.boxplot(data=df, y='word_count')
plt.title("Word Count in Abstracts")
plt.show()

print(f"Mean of Word Count: {df['word_count'].mean():.2f}\nMedian of Word Count: {df['word_count'].median()}")

"""## Universal Sentence Encoder

"""

# Tensorflow Hub URL for Universal Sentence Encoder
MODEL_URL = "https://tfhub.dev/google/universal-sentence-encoder/4"

import tensorflow_hub as hub
import tensorflow as tf


# KerasLayer
sentence_encoder_layer = hub.KerasLayer(MODEL_URL,
                                        input_shape=[],
                                        dtype=tf.string,
                                        trainable=False,
                                        name="use")

abstracts = df["abstract"].to_list()

# Setup for embeddings computation
embeddings = []
batch_size = 300
num_batches = len(abstracts) // batch_size

# Compute Embeddings in batches
for i in range(num_batches):
    batch_abstracts = abstracts[i*batch_size : (i+1)*batch_size]
    batch_embeddings = sentence_encoder_layer(batch_abstracts)
    embeddings.extend(batch_embeddings.numpy())

# Embeddings for remaining abstracts
remaining_abstracts = abstracts[num_batches*batch_size:]
if len(remaining_abstracts) > 0:
    remaining_embeddings = sentence_encoder_layer(remaining_abstracts)
    embeddings.extend(remaining_embeddings.numpy())

embeddings = np.array(embeddings)
y = df.index

"""### KNN Classifier"""

nn = KNeighborsClassifier(n_neighbors=6)
nn.fit(embeddings,y)

"""### Visualize Results"""

for _ in range(5):
    idx = random.randint(i, len(y))
    sample = df["title"][idx]
    dist, index = nn.kneighbors(X=embeddings[idx,:].reshape(1,-1))
    print(f"Sample:\n{sample}\n")
    for i in range(1,6):
        print(f"Recommendation {i}:\n{df['title'][index[0][i]]}\n")
    print("===============\n")

"""# STREAMLIT CODE"""
import sys
sys.path.append("/home/appuser/.local/lib/python3.12/site-packages")

import os
os.system("pip install streamlit")

os.system("pip install pyvis")

os.system("pip install networkx")

os.system("pip install scikit-learn")

import streamlit as st
import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation as LDA
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.neighbors import NearestNeighbors
from pyvis.network import Network
import numpy as np

# Title and Sidebar
st.title("Insight Xplore: Advanced Research Paper Analysis")
st.sidebar.title("Navigation")
options = ["Upload Dataset", "Topic Modelling (LDA)", "Abstract Clustering", "Citation Network Analysis", "Research Paper Recommendations"]
selection = st.sidebar.selectbox("Choose a feature", options)

# Global variables
uploaded_file = None
df = None

# Function to upload dataset
if selection == "Upload Dataset":
    st.sidebar.subheader("Upload Research Papers Dataset")
    uploaded_file = st.sidebar.file_uploader("Upload a CSV file", type="csv")
    if uploaded_file:
        df = pd.read_csv(uploaded_file)
        st.write("### Dataset Preview:")
        st.write(df.head())
        st.session_state['df'] = df  # Store in session state for later access
else:
    df = st.session_state.get('df', None)
    if df is None:
        st.warning("Please upload a dataset first.")

# Topic Modelling (LDA)
if selection == "Topic Modelling (LDA)" and df is not None:
    st.header("Topic Modelling with LDA")
    text_column = st.selectbox("Select the column containing abstracts:", df.columns)
    num_topics = st.slider("Number of Topics:", min_value=2, max_value=10, value=5)

    if st.button("Run LDA"):
        vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')
        dtm = vectorizer.fit_transform(df[text_column])
        lda_model = LDA(n_components=num_topics, random_state=42)
        lda_model.fit(dtm)

        # Display topics
        topics = {f"Topic {i+1}": [vectorizer.get_feature_names_out()[idx] for idx in topic.argsort()[-10:]] for i, topic in enumerate(lda_model.components_)}
        st.write("### Discovered Topics:")
        st.write(pd.DataFrame(topics))

# Abstract Clustering
if selection == "Abstract Clustering" and df is not None:
    st.header("Abstract Clustering")
    text_column = st.selectbox("Select the column for clustering:", df.columns)
    num_clusters = st.slider("Number of Clusters:", min_value=2, max_value=10, value=5)

    if st.button("Run Clustering"):
        tfidf = TfidfVectorizer(stop_words='english')
        tfidf_matrix = tfidf.fit_transform(df[text_column].fillna(''))
        kmeans = KMeans(n_clusters=num_clusters, random_state=42)
        clusters = kmeans.fit_predict(tfidf_matrix)

        # Add cluster labels to dataframe
        df['Cluster'] = clusters
        st.write("### Clustered Data:")
        st.write(df[['Cluster', text_column]].head())

# Citation Network Analysis
if selection == "Citation Network Analysis" and df is not None:
    st.header("Citation Network Analysis")
    st.write("The dataset must contain 'source' and 'target' columns for citations.")
    if "source" in df.columns and "target" in df.columns:
        G = nx.from_pandas_edgelist(df, source="source", target="target")
        pagerank = nx.pagerank(G)

        st.write("### Top Papers by PageRank:")
        top_papers = sorted(pagerank.items(), key=lambda x: x[1], reverse=True)[:10]
        st.write(pd.DataFrame(top_papers, columns=["Paper", "PageRank"]))

        # Visualize the network
        net = Network(height='500px', width='100%', bgcolor='#222222', font_color='white')
        net.from_nx(G)
        net.show("citation_network.html")
        st.write("### Citation Network Visualization:")
        st.markdown(f'<iframe src="citation_network.html" width="100%" height="500"></iframe>', unsafe_allow_html=True)
    else:
        st.warning("The dataset does not have 'source' and 'target' columns.")

# Research Paper Recommendations
if selection == "Research Paper Recommendations" and df is not None:
    st.header("Research Paper Recommendations")
    text_column = st.selectbox("Select the column containing text for similarity:", df.columns)
    paper_id = st.number_input("Enter a Paper ID for Recommendation:", min_value=0, max_value=len(df)-1, step=1)
    num_recommendations = st.slider("Number of Recommendations:", min_value=1, max_value=10, value=5)

    if st.button("Get Recommendations"):
        tfidf = TfidfVectorizer(stop_words='english')
        tfidf_matrix = tfidf.fit_transform(df[text_column].fillna(''))
        cosine_sim = cosine_similarity(tfidf_matrix)
        similar_indices = np.argsort(-cosine_sim[paper_id])[:num_recommendations+1]

        recommendations = df.iloc[similar_indices[1:]]  # Exclude the input paper itself
        st.write("### Recommended Papers:")
        st.write(recommendations)

st.sidebar.write("---")
st.sidebar.write("Created with ❤️ by Priyanshi Gupta")
