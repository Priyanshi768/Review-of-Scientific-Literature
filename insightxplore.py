# -*- coding: utf-8 -*-
"""InsightXplore.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Khnwd25b6544O39AcecaJZE_z0IQrm1O
"""


import sys
sys.path.append("/home/appuser/.local/lib/python3.12/site-packages")

import os
os.system("pip install streamlit")

os.system("pip install pyvis")

os.system("pip install networkx")

os.system("pip install scikit-learn")

os.system("pip install PyPDF2")


import streamlit as st
import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation as LDA
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.neighbors import NearestNeighbors
from pyvis.network import Network
import numpy as np

# Title and Sidebar
st.title("Insight Xplore: Advanced Research Paper Analysis")
st.sidebar.title("Navigation")
options = ["Upload Dataset", "Topic Modelling (LDA)", "Abstract Clustering", "Citation Network Analysis", "Research Paper Recommendations"]
selection = st.sidebar.selectbox("Choose a feature", options)

# Global variables
uploaded_file = None
df = None

import pandas as pd
import streamlit as st
from PyPDF2 import PdfReader

# Function to handle dataset upload and processing
if selection == "Upload Dataset":
    st.sidebar.subheader("Upload Research Papers Dataset")
    uploaded_file = st.sidebar.file_uploader("Upload a file (CSV, XLS, XLSX, JSON, PDF)", type=["csv", "xls", "xlsx", "json", "pdf"])
    
    if uploaded_file:
        file_extension = uploaded_file.name.split(".")[-1].lower()

        if file_extension == "csv":
            df = pd.read_csv(uploaded_file)
        elif file_extension in ["xls", "xlsx"]:
            df = pd.read_excel(uploaded_file)
        elif file_extension == "json":
            df = pd.read_json(uploaded_file)
        elif file_extension == "pdf":
            pdf_reader = PdfReader(uploaded_file)
            pdf_text = ""
            for page in pdf_reader.pages:
                pdf_text += page.extract_text()
            st.text_area("Extracted Text from PDF:", pdf_text, height=300)
            st.warning("PDFs are displayed as text and cannot be previewed as a structured dataset.")
            df = None
        else:
            st.error("Unsupported file type.")
            df = None

        if df is not None:
            st.write("### Dataset Preview:")
            st.write(df.head())
            st.session_state['df'] = df  # Store in session state for later access
    else:
        st.warning("Please upload a file.")
else:
    df = st.session_state.get('df', None)
    if df is None:
        st.warning("Please upload a dataset first.")


# Topic Modelling (LDA)
if selection == "Topic Modelling (LDA)" and df is not None:
    st.header("Topic Modelling with LDA")
    text_column = st.selectbox("Select the column containing abstracts:", df.columns)
    num_topics = st.slider("Number of Topics:", min_value=2, max_value=10, value=5)

    if st.button("Run LDA"):
        vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')
        dtm = vectorizer.fit_transform(df[text_column])
        lda_model = LDA(n_components=num_topics, random_state=42)
        lda_model.fit(dtm)

        # Display topics
        topics = {f"Topic {i+1}": [vectorizer.get_feature_names_out()[idx] for idx in topic.argsort()[-10:]] for i, topic in enumerate(lda_model.components_)}
        st.write("### Discovered Topics:")
        st.write(pd.DataFrame(topics))

# Abstract Clustering
if selection == "Abstract Clustering" and df is not None:
    st.header("Abstract Clustering")
    text_column = st.selectbox("Select the column for clustering:", df.columns)
    num_clusters = st.slider("Number of Clusters:", min_value=2, max_value=10, value=5)

    if st.button("Run Clustering"):
        tfidf = TfidfVectorizer(stop_words='english')
        tfidf_matrix = tfidf.fit_transform(df[text_column].fillna(''))
        kmeans = KMeans(n_clusters=num_clusters, random_state=42)
        clusters = kmeans.fit_predict(tfidf_matrix)

        # Add cluster labels to dataframe
        df['Cluster'] = clusters
        st.write("### Clustered Data:")
        st.write(df[['Cluster', text_column]].head())

import networkx as nx
from pyvis.network import Network
import pandas as pd
import streamlit as st

if selection == "Citation Network Analysis" and df is not None:
    st.header("Citation Network Analysis")

    # Select the type of network to analyze
    network_type = st.selectbox("Select the type of network:", ["Citation Network", "Author Collaboration Network", "Claps Network"])

    if network_type == "Citation Network":
        st.write("The dataset must contain 'source' and 'target' columns for citations.")
        if "source" in df.columns and "target" in df.columns:
            # Build the citation network
            G = nx.from_pandas_edgelist(df, source="source", target="target")
            pagerank = nx.pagerank(G)

            st.write("### Top Papers by PageRank:")
            top_papers = sorted(pagerank.items(), key=lambda x: x[1], reverse=True)[:10]
            st.write(pd.DataFrame(top_papers, columns=["Paper", "PageRank"]))

            # Visualize the network
            net = Network(height='500px', width='100%', bgcolor='#222222', font_color='white')
            net.from_nx(G)
            net.show("citation_network.html")
            net.show("author_network.html")
st.markdown(f'<iframe src="author_network.html" width="100%" height="500"></iframe>', unsafe_allow_html=True)

            st.write("### Citation Network Visualization:")
            st.markdown(f'<iframe src="citation_network.html" width="100%" height="500"></iframe>', unsafe_allow_html=True)
        else:
            st.warning("The dataset does not have 'source' and 'target' columns.")

    elif network_type == "Author Collaboration Network":
        st.write("The dataset must contain an 'author' column with comma-separated author names.")
        if "author" in df.columns:
            # Build the author collaboration network
            author_edges = []
            for authors in df["author"]:
                author_list = [a.strip() for a in authors.split(",")]
                author_edges.extend([(author_list[i], author_list[j]) for i in range(len(author_list)) for j in range(i + 1, len(author_list))])
            G = nx.Graph()
            G.add_edges_from(author_edges)

            degree = nx.degree_centrality(G)
            st.write("### Top Authors by Degree Centrality:")
            top_authors = sorted(degree.items(), key=lambda x: x[1], reverse=True)[:10]
            st.write(pd.DataFrame(top_authors, columns=["Author", "Degree Centrality"]))

            # Visualize the network
            net = Network(height='500px', width='100%', bgcolor='#222222', font_color='white')
            net.from_nx(G)
            net.show("author_network.html")
            st.write("### Author Collaboration Network Visualization:")
            st.markdown(f'<iframe src="author_network.html" width="100%" height="500"></iframe>', unsafe_allow_html=True)
        else:
            st.warning("The dataset does not have an 'author' column.")

    elif network_type == "Claps Network":
        st.write("The dataset must contain 'clap_source' and 'clap_target' columns for claps-based connections.")
        if "clap_source" in df.columns and "clap_target" in df.columns:
            # Build the claps-based network
            G = nx.from_pandas_edgelist(df, source="clap_source", target="clap_target")
            pagerank = nx.pagerank(G)

            st.write("### Top Nodes by PageRank:")
            top_nodes = sorted(pagerank.items(), key=lambda x: x[1], reverse=True)[:10]
            st.write(pd.DataFrame(top_nodes, columns=["Node", "PageRank"]))

            # Visualize the network
            net = Network(height='500px', width='100%', bgcolor='#222222', font_color='white')
            net.from_nx(G)
            net.show("clap_network.html")
            st.write("### Claps Network Visualization:")
            st.markdown(f'<iframe src="clap_network.html" width="100%" height="500"></iframe>', unsafe_allow_html=True)
        else:
            st.warning("The dataset does not have 'clap_source' and 'clap_target' columns.")

# Research Paper Recommendations
if selection == "Research Paper Recommendations" and df is not None:
    st.header("Research Paper Recommendations")
    text_column = st.selectbox("Select the column containing text for similarity:", df.columns)
    paper_id = st.number_input("Enter a Paper ID for Recommendation:", min_value=0, max_value=len(df)-1, step=1)
    num_recommendations = st.slider("Number of Recommendations:", min_value=1, max_value=10, value=5)

    if st.button("Get Recommendations"):
        tfidf = TfidfVectorizer(stop_words='english')
        tfidf_matrix = tfidf.fit_transform(df[text_column].fillna(''))
        cosine_sim = cosine_similarity(tfidf_matrix)
        similar_indices = np.argsort(-cosine_sim[paper_id])[:num_recommendations+1]

        recommendations = df.iloc[similar_indices[1:]]  # Exclude the input paper itself
        st.write("### Recommended Papers:")
        st.write(recommendations)

st.sidebar.write("---")
st.sidebar.write("Created with ❤️ by Priyanshi Gupta")
